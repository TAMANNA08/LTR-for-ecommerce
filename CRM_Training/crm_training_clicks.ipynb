{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = ''\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "\n",
    "import sys\n",
    "from crm_model import crm_model\n",
    "from utils import batch_gen\n",
    "from evaluation_metrics import get_trec_eval_metrics\n",
    "from tensorflow.keras.backend import set_session\n",
    "\n",
    "import random\n",
    "from tensorflow import set_random_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kinldy be sure that the evaluation tool trec_eval is installed. Instructions \n",
    "# can be found at: https://github.com/usnistgov/trec_eval\n",
    "# Provide the  absolute path of the tool here:\n",
    "TREC_EVAL_PATH = \"/trec_eval\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "EPOCHS = 5\n",
    "MODEL_DIR = ''\n",
    "LOG_DIR = ''\n",
    "LAGRANGE_MULTIPLIER = 0.02\n",
    "INPUT_DATA_PATH = ''\n",
    "Supervised_files_PATH = 'Supervised_ClickNRR_files/'\n",
    "Embeddings_Index_PATH = 'Embeddings_Index_files/'\n",
    "MODEL_NAME = 'crm_clicks_logs'\n",
    "BANDIT_FEEDBACK_UPDATE = 10000 #number of bandit-feedback for model update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "train_data = pd.read_csv(os.path.join(INPUT_DATA_PATH, 'logs/click_logs.csv'))\n",
    "dev_data = pd.read_csv(os.path.join(Supervised_files_PATH, 'Supervised_Dev_ClickNRR_File.csv'))\n",
    "test_data = pd.read_csv(os.path.join(Supervised_files_PATH, 'Supervised_Test_ClickNRR_File.csv'))\n",
    "\n",
    "qids_train = train_data['qid']\n",
    "loss_train = train_data['loss']\n",
    "action_train = train_data['action']\n",
    "addn_feat_train = np.array(train_data['addn_feat'].apply(lambda x: [float(elem) for elem in x.split(', ')]).tolist())\n",
    "probs_train = train_data['control_policy_prob']\n",
    "\n",
    "qids_dev = dev_data['qids']\n",
    "addn_feat_dev = np.array(dev_data['addn_feat'].apply(lambda x: [float(elem) for elem in x.split(', ')]).tolist())\n",
    "\n",
    "qids_test = test_data['qids']\n",
    "addn_feat_test = np.array(test_data['addn_feat'].apply(lambda x: [float(elem) for elem in x.split(', ')]).tolist())\n",
    "\n",
    "# Load embeddings matrix\n",
    "embeddings = np.load(os.path.join(INPUT_DATA_PATH, 'embedding.npy'))\n",
    "\n",
    "# Load embeddings indices for queries and product titles\n",
    "\n",
    "# Each row in q_train corresponds to the query of the sample in train_data. Both are aligned.\n",
    "# q_train contain numpy arrays of query word indices used for lookup in embedding matrix. \n",
    "q_train = np.load(os.path.join(Embeddings_Index_PATH, 'click_logs_queries.npy'))\n",
    "\n",
    "# Each row in a_train corresponds to the title of product in train_data. Both are aligned.\n",
    "# a_train contain numpy arrays of word indices of the product title, these indices are used for lookup in embedding matrix. \n",
    "a_train = np.load(os.path.join(Embeddings_Index_PATH, 'click_logs_products.npy'))\n",
    "\n",
    "q_dev = np.load(os.path.join(Embeddings_Index_PATH, 'queries_dev.npy'))\n",
    "a_dev = np.load(os.path.join(Embeddings_Index_PATH, 'products_dev.npy'))\n",
    "\n",
    "q_test = np.load(os.path.join(Embeddings_Index_PATH, 'queries_test.npy'))\n",
    "a_test = np.load(os.path.join(Embeddings_Index_PATH, 'products_test.npy'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose variant of relevance label here:\n",
    "\n",
    "# binary-ceiled relevance label\n",
    "y_dev = np.ceil(dev_data['click_NRR']).astype(int)\n",
    "y_test = np.ceil(test_data['click_NRR']).astype(int)\n",
    "\n",
    "# binary-roundedoff relevance label\n",
    "\n",
    "# y_dev = np.round(dev_data['click_NRR']).astype(int)\n",
    "# y_test = np.round(test_data['click_NRR']).astype(int)\n",
    "\n",
    "# graded-ceiled relevance label\n",
    "\n",
    "# max_relevance_grade = 4\n",
    "# y_dev = np.ceil(dev_data['click_NRR']*max_relevance_grade).astype(int)\n",
    "# y_test = np.ceil(test_data['click_NRR']*max_relevance_grade).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixing random seed for ease of reproducibility of results\n",
    "np.random.seed(2) \n",
    "random.seed(2) \n",
    "set_random_seed(2)\n",
    "\n",
    "addit_feat_len = addn_feat_train.shape[1]\n",
    "embed_dim = embeddings.shape\n",
    "max_ques_len = q_train.shape[1]\n",
    "max_ans_len = a_train.shape[1]\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    with tf.Session() as sess:\n",
    "        set_session(sess)\n",
    "\n",
    "        # Create instance of model\n",
    "        cnn_model_instance = crm_model(max_ques_len, max_ans_len,  embeddings, addit_feat_len=addit_feat_len)\n",
    "\n",
    "        # Compute weights for the model\n",
    "        weights_train = (loss_train - LAGRANGE_MULTIPLIER)/probs_train \n",
    "\n",
    "        y_pred_dev = cnn_model_instance.predict([q_dev, a_dev, addn_feat_dev, np.ones(shape = len(q_dev))])\n",
    "        map_score_overall, mrr, p_5, p_10, ndcg_5, ndcg_10 = get_trec_eval_metrics(qids_dev, y_pred_dev, y_dev, TREC_EVAL_PATH)\n",
    "        print('Initial results on {} set are: MAP: {}, MRR:{}, P@5: {}, P@10: {}, NDCG@5: {}, NDCG@10: {}'\n",
    "              .format('DEV', map_score_overall, mrr, p_5, p_10, ndcg_5, ndcg_10))\n",
    "        y_pred_test = cnn_model_instance.predict([q_test, a_test, addn_feat_test, np.ones(shape = len(q_test))])\n",
    "        map_score, mrr, p_5, p_10, ndcg_5, ndcg_10 = get_trec_eval_metrics(qids_test, y_pred_test, y_test, TREC_EVAL_PATH)\n",
    "        print('Initial results on {} set are: MAP: {}, MRR:{}, P@5: {}, P@10: {}, NDCG@5: {}, NDCG@10: {}'\n",
    "              .format('TEST', map_score, mrr, p_5, p_10, ndcg_5, ndcg_10))\n",
    "\n",
    "        best_model_weights = cnn_model_instance.get_weights()\n",
    "        ndcg_10_overall = 0\n",
    "        \n",
    "        for epoch in range(EPOCHS):\n",
    "            print('Epoch: ', epoch)\n",
    "\n",
    "            BANDIT_FEEDBACK = 0\n",
    "            for b_q_train, b_a_train, b_addn_feat_train, b_weights, b_action, b_qid in zip(\n",
    "                batch_gen(q_train, BATCH_SIZE), batch_gen(a_train, BATCH_SIZE), \n",
    "                batch_gen(addn_feat_train, BATCH_SIZE), batch_gen(weights_train, BATCH_SIZE),\n",
    "                batch_gen(action_train, BATCH_SIZE), batch_gen(qids_train, BATCH_SIZE)):\n",
    "\n",
    "                cnn_model_instance.train_on_batch([b_q_train, b_a_train, b_addn_feat_train, b_weights], b_action)\n",
    "                \n",
    "                BANDIT_FEEDBACK += 1\n",
    "                if BANDIT_FEEDBACK%BANDIT_FEEDBACK_UPDATE == 0:\n",
    "\n",
    "                    print('{} BANDIT FEEDBACKS were already processed'.format(BANDIT_FEEDBACK*BATCH_SIZE))   \n",
    "\n",
    "                    y_pred_dev = cnn_model_instance.predict([q_dev, a_dev, addn_feat_dev, np.ones(shape = len(q_dev))])\n",
    "                    map_score, mrr, p_5, p_10, ndcg_5, ndcg_10 = get_trec_eval_metrics(qids_dev, y_pred_dev, y_dev, TREC_EVAL_PATH)\n",
    "                    if ndcg_10 > ndcg_10_overall:\n",
    "                        print ('Model Weights Updated')\n",
    "                        print ('Results on {} set are: MAP: {}, MRR:{}, NDCG@5: {}, NDCG@10: {}'.format('DEV', map_score, mrr, ndcg_5, ndcg_10))\n",
    "                        ndcg_10_overall = ndcg_10\n",
    "                        best_model_weights = cnn_model_instance.get_weights()\n",
    "\n",
    "        cnn_model_instance.set_weights(best_model_weights)\n",
    "        cnn_model_instance.save(os.path.join(MODEL_DIR, MODEL_NAME+'.h5'))\n",
    "\n",
    "        y_pred_dev = cnn_model_instance.predict([q_dev, a_dev, addn_feat_dev, np.ones(shape = len(q_dev))])\n",
    "        map_score, mrr, p_5, p_10, ndcg_5, ndcg_10 = get_trec_eval_metrics(qids_dev, y_pred_dev, y_dev, TREC_EVAL_PATH)\n",
    "        print('BEST results on {} set are: MAP: {}, MRR:{}, P@5: {}, P@10: {}, NDCG@5: {}, NDCG@10: {}'\n",
    "              .format('DEV', map_score, mrr, p_5, p_10, ndcg_5, ndcg_10))\n",
    "        y_pred_test = cnn_model_instance.predict([q_test, a_test, addn_feat_test, np.ones(shape = len(q_test))])\n",
    "        map_score, mrr, p_5, p_10, ndcg_5, ndcg_10 = get_trec_eval_metrics(qids_test, y_pred_test, y_test, TREC_EVAL_PATH)\n",
    "        print('BEST results on {} set are: MAP: {}, MRR:{}, P@5: {}, P@10: {}, NDCG@5: {}, NDCG@10: {}'\n",
    "              .format('TEST', map_score, mrr, p_5, p_10, ndcg_5, ndcg_10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
